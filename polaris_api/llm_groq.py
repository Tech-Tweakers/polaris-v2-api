from groq import Groq
from polaris_logger import log_info, log_success, log_warning, log_error


class GroqLLM:
    def __init__(self, api_key: str, model: str = "llama-3.3-70b-versatile"):
        self.api_key = api_key
        self.model = model

    def load(self):
        log_info("ğŸ”Œ Polaris conectado ao backend remoto.")
        log_success(f"âœ… Modelo configurado: {self.model}")

    def close(self):
        log_info("ğŸ›‘ Encerrando conexÃ£o simbÃ³lica com o backend remoto.")

    def invoke(self, prompt: str) -> str:
        """MÃ©todo sÃ­ncrono para compatibilidade"""
        return self.invoke_stream(prompt, lambda chunk: None)

    def invoke_stream(self, prompt: str, stream_callback=None) -> str:
        """MÃ©todo com suporte a streaming"""
        client = Groq(api_key=self.api_key)

        try:
            log_info(f"ğŸ“¤ Enviando prompt para o backend remoto...")

            # Para streaming, usa stream=True
            if stream_callback:
                log_info("ğŸ¬ Iniciando streaming...")
                chat_completion = client.chat.completions.create(
                    messages=[
                        {
                            "role": "system",
                            "content": "VocÃª Ã© Polaris, um assistente inteligente.",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    model=self.model,
                    stream=True,  # Habilita streaming
                    temperature=0.3,
                    max_tokens=1024,
                )

                full_content = ""
                chunk_count = 0
                for chunk in chat_completion:
                    chunk_count += 1
                    log_info(f"ğŸ“¦ Chunk {chunk_count} recebido")
                    if hasattr(chunk.choices[0].delta, 'content') and chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        log_info(f"ğŸ“ ConteÃºdo: '{content}'")
                        full_content += content
                        stream_callback(content)  # Chama callback com chunk
                    else:
                        log_info(f"ğŸ“¦ Chunk sem conteÃºdo: {chunk}")

                log_success(f"ğŸ§  Streaming concluÃ­do com {chunk_count} chunks. ConteÃºdo final: {len(full_content)} chars")
                return full_content

            else:
                # Modo nÃ£o-streaming (compatibilidade)
                chat_completion = client.chat.completions.create(
                    messages=[
                        {
                            "role": "system",
                            "content": "VocÃª Ã© Polaris, um assistente inteligente.",
                        },
                        {"role": "user", "content": prompt},
                    ],
                    model=self.model,
                    temperature=0.3,
                    max_tokens=1024,
                )

                content = chat_completion.choices[0].message.content
                log_success(f"ğŸ§  Resposta remota recebida com sucesso.")
                return content

        except Exception as e:
            log_error(f"âŒ Erro na inferÃªncia via backend remoto: {e}")
            return "Erro ao consultar o modelo remoto. Tente novamente em alguns instantes."
